{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c57f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(r'd:\\\\GyanPrakashKuswaha\\\\GenAI\\\\InternOps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc682903",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = \"\"\"\n",
    "Gyan Prakash Kushwaha\n",
    "/githubGitHub| /linkedinLinkedIn| /gl⌢beKaggle| /envel⌢pegyanprakash.sde| ♂¶obile+91 9575765381\n",
    "SUMMARY\n",
    "AI Engineering Undergraduate (IIT Madras) with strong DSA fundamentals (200+ LeetCodeproblems). Experi-\n",
    "enced in buildingagentic RAG workflowsand end-to-end ML systems usingLangGraph, FastAPI, and Vector\n",
    "DBs. Proficient in reducing inference latency and deploying scalable AI solutions. Seeking an SDE/AI internship to\n",
    "leverage skills in Generative AI and backend optimization.\n",
    "EDUCATION\n",
    "Indian Institute of Technology (IIT), MadrasChennai, Tamil Nadu\n",
    "BS in Data Science and Applications; CGPA: 8.18/10 2023 – 2027\n",
    "SKILLS\n",
    "Programming Languages Python, JavaScript (ES6+), Java, SQL\n",
    "Backend Frameworks Flask, FastAPI, Node.js\n",
    "Frontend Technologies React, Vue.js, Bootstrap, Tailwind CSS\n",
    "Databases & Storage Systems PostgreSQL, SQLite, DuckDB, Redis\n",
    "Vector Databases & Search FAISS, ChromaDB\n",
    "Machine Learning & NLP NumPy, Pandas, Scikit-learn, PyTorch, TensorFlow, NLTK\n",
    "Data Visualization Power BI, Matplotlib, Seaborn, Plotly\n",
    "Generative AI & LLM Frameworks LangChain, LangGraph, LangSmith, AGNO\n",
    "DevOps & Systems Linux, Git, GitHub, Celery\n",
    "Cloud Platforms AWS (EC2, S3, Lambda), GCP\n",
    "PROJECTS\n",
    "YouTube RAG Chatbot|LangGraph, FastAPI, FAISS, AsyncSQLite, Vue.js, Gemini (Flash-Lite) Repo Link\n",
    "– ReducedTime-to-First-Token (TTFT) by 90%(from 5s to<500ms) by engineering a real-time async streaming\n",
    "pipeline withFastAPIandLangGraph, significantly improving user perceived responsiveness.\n",
    "– Architected astateful RAG workflowusingLangGraphandAsyncSQLiteto orchestrate persistent, multi-turn\n",
    "conversations, enabling context coherence across1-hour+ video transcripts.\n",
    "– Increased RAG response accuracy by implementingMaximal Marginal Relevance (MMR)search, filtering\n",
    "90% of redundant contextfrom long-form audio transcripts to minimizeLLM hallucinations.\n",
    "CropGuardian-AI|LangGraph, FastAPI, Pydantic, Gemini 1.5 Pro, OpenMeteo Repo Link\n",
    "– Architected amulti-agent workflowusingLangGraphto orchestrate3 specialized agents(Vision, Weather,\n",
    "Agronomy), automating the complex reasoning chain from image analysis to actionable advice with<3s latency.\n",
    "– Implemented strictStructured Outputenforcement usingPydanticandGemini 1.5 Pro, achieving100%\n",
    "schema compliancefor JSON responses and eliminating parsing errors in the frontend application.\n",
    "– Designed acontext-aware reasoning enginethat grounds visual diagnosis in real-time environmental data\n",
    "(OpenMeteo API), generating location-specific farming action plans based on7-day weather forecasts.\n",
    "Customer Churn Predictor|Python, Scikit-learn, XGBoost, MLflow, Streamlit Repo Link\n",
    "– Developed a modular machine learning pipeline to process100,000 customer records, benchmarking perfor-\n",
    "mance across4 ensemble algorithms(XGBoost, Random Forest, AdaBoost, Gradient Boosting) to identify op-\n",
    "timal churn predictors.\n",
    "– Engineered a robust training workflow integrated withMLflowto systematically track model metrics, parameters,\n",
    "and version history, ensuringreproducibilityacross experimentation cycles.\n",
    "– Deployed the best-performing model as an interactive web application usingStreamlit, enabling non-technical\n",
    "stakeholders to input customer demographics and generatereal-time churn risk assessments.\n",
    "Movie Recommender System|Python, Scikit-Learn, Streamlit, Pandas Repo Link\n",
    "–Developed a Content-Based Movie Recommender SystemusingPythonandStreamlit, processing a dataset\n",
    "of4,800+ moviesto generate personalized top-10 viewing suggestions.\n",
    "–Engineered a feature extraction pipelinewithPandasandScikit-Learn, transforming unstructured metadata\n",
    "(genres, cast, crew) into a5,000-featureBag-of-Words model to calculateCosine Similarityscores.\n",
    "–Deployed an interactive web applicationintegrating theTMDB APIto fetch real-time posters, utilizingPickle\n",
    "serialization to optimize data loading and deliver recommendations efficiently.\n",
    "ACHIEVEMENTS\n",
    "•Kaggle Expert: Top 4% globally (Rank 341), 1 Silver & 9 Bronze medals; datasets with 22K+ views and 5.6K+\n",
    "downloads\n",
    "•LeetCode: Solved 219+ DSA problems (100+ Medium)\n",
    "•HackerRank: 5⋆Gold Badge (SQL)\n",
    "    \"\"\"\n",
    "    \n",
    "analysis_output = {\n",
    "  \"ats_result\": {\n",
    "    \"match_score\": 85,\n",
    "    \"missing_keywords\": [\n",
    "      \"LLMs (Open AI, LLaMA, Mistral, etc.)\",\n",
    "      \"Fine-tuning / PEFT concepts\"\n",
    "    ],\n",
    "    \"formatting_issues\": [],\n",
    "    \"decision\": \"PASS\",\n",
    "    \"feedback\": \"The resume has a good match with the JD. However, it is missing specific mentions of LLM models like OpenAI, LLaMA, Mistral, and fine-tuning concepts like PEFT. Adding these to the skills or projects section would strengthen the application further. The resume is well-formatted and easy to parse.\"\n",
    "  },\n",
    "  \"recruiter_result\": {\n",
    "    \"career_progression_score\": 95,\n",
    "    \"red_flags\": [],\n",
    "    \"soft_skills_detected\": [\n",
    "      \"analytical thinking\",\n",
    "      \"problem-solving skills\"\n",
    "    ],\n",
    "    \"decision\": \"PASS\",\n",
    "    \"feedback\": \"The candidate has a strong academic background and relevant project experience. The resume is well-structured and highlights key skills and achievements effectively. While the ATS noted a lack of specific LLM model mentions, the overall profile is very promising. Recommend moving forward.\"\n",
    "  },\n",
    "  \"hm_result\": {\n",
    "    \"tech_depth_score\": 75,\n",
    "    \"project_impact_score\": 60,\n",
    "    \"stack_alignment\": \"Good\",\n",
    "    \"decision\": \"MAYBE\",\n",
    "    \"interview_questions\": [\n",
    "      \"Can you elaborate on your experience with fine-tuning LLMs? Specifically, what PEFT techniques have you used, and what were the challenges you faced in terms of data preparation and evaluation?\",\n",
    "      \"Describe a situation where you had to explain a complex AI concept (like RAG or prompt engineering) to someone with a non-technical background. What approach did you take, and what was the outcome?\",\n",
    "      \"You mention building 'production-ready ML pipelines'. Can you walk me through the architecture of one such pipeline, highlighting the components for data ingestion, preprocessing, model training, deployment, and monitoring?\",\n",
    "      \"The JD emphasizes 'AI application architecture'. Can you discuss a project where you were responsible for the overall architecture of an AI application, detailing the trade-offs you considered in your design choices?\"\n",
    "    ],\n",
    "    \"feedback\": \"The candidate has a solid foundation in ML and data science, with good project experience in areas relevant to the JD, such as churn prediction and recommendation systems. Their YouTube RAG Chatbot project demonstrates experience with modern Generative AI concepts like RAG, LangGraph, and FastAPI, which aligns well with the JD's requirements. The candidate also shows good impact in terms of latency reduction (90% TTFT). However, the JD specifically asks for experience with LLMs like OpenAI, LLaMA, Mistral, and fine-tuning/PEFT concepts. While the candidate mentions RAG and Gemini (Flash-Lite), direct experience with fine-tuning specific LLMs or a broader range of LLM providers isn't explicitly detailed in the projects. The 'Stack Compliance' is rated 'Good' because the core technologies like Python, Scikit-learn, FastAPI, and concepts like RAG are present, but there's room for deeper alignment on specific LLM models and fine-tuning. To improve, the candidate should quantify the impact of their projects more, if possible (e.g., 'improved recommendation accuracy by X%', 'reduced false positives in churn prediction by Y%'). For the 'AI Instructor / Generative AI Mentor' role, emphasizing the teaching and mentoring aspects more explicitly in their summary or project descriptions would be beneficial. For instance, how they broke down complex concepts for others or mentored junior team members.\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b89aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'latex_resume_code': '\\\\documentclass[a4paper,12pt]{article}\\n\\n\\\\usepackage{mathptmx}\\n\\\\usepackage{url}\\n\\\\usepackage{parskip}\\n\\\\RequirePackage{color}\\n\\\\RequirePackage{graphicx}\\n\\\\usepackage[usenames,dvipsnames]{xcolor}\\n\\\\usepackage[top=0.2in, bottom=0.3in, left=0.3in, right=0.3in]{geometry}\\n\\\\usepackage{tabularx}\\n\\\\usepackage{enumitem}\\n\\\\usepackage{supertabular}\\n\\\\usepackage{titlesec}\\n\\\\usepackage{multicol}\\n\\\\usepackage{multirow}\\n\\\\usepackage{fontawesome5}\\n\\n\\\\newcolumntype{C}{>{\\\\centering\\\\arraybackslash}X}\\n\\n\\\\titleformat{\\\\section}{\\\\Large\\\\scshape\\\\raggedright}{}{0em}{}[\\\\titlerule]\\n\\\\titlespacing{\\\\section}{0pt}{5pt}{5pt}\\n\\n\\\\usepackage[unicode, draft=false]{hyperref}\\n\\\\definecolor{linkcolour}{rgb}{0,0.2,0.6}\\n\\\\hypersetup{colorlinks,breaklinks,urlcolor=linkcolour,linkcolor=linkcolour}\\n\\n% Custom Environments\\n\\\\newenvironment{jobshort}[2]\\n    {\\n    \\\\begin{tabularx}{\\\\linewidth}{@{}l X r@{}}\\n    \\\\textbf{#1} & \\\\hfill &  #2 \\\\\\\\\\\\\\\\[2pt]\\n    \\\\end{tabularx}\\n    }\\n    {}\\n\\n\\\\newenvironment{joblong}[2]\\n    {\\n    \\\\begin{tabularx}{\\\\linewidth}{@{}X r@{}}\\n    \\\\textbf{#1} & #2 \\\\\\\\\\\\\\\\[2pt]\\n    \\\\end{tabularx}\\n    \\\\begin{minipage}[t]{\\\\linewidth}\\n    \\\\begin{itemize}[nosep,after=\\\\strut, leftmargin=1em, itemsep=1pt,label=--]\\n    }\\n    {\\n    \\\\end{itemize}\\n    \\\\end{minipage}\\n    }\\n\\n\\\\begin{document}\\n\\\\pagestyle{empty}\\n\\n% HEADER\\n\\\\begin{tabularx}{\\\\linewidth}{@{} C @{}}\\n\\\\Huge{Gyan Prakash Kushwaha} \\\\\\\\\\\\\\\\[5pt]\\n\\\\href{https://github.com/gyanprakashkushwaha}{\\\\faGithub\\\\ GitHub} \\\\ $|$ \\\\\\n\\\\href{https://www.linkedin.com/in/gyanprakashkushwaha}{\\\\faLinkedin\\\\ LinkedIn} \\\\ $|$ \\\\\\n\\\\href{mailto:gyanprakash.sde@gmail.com}{\\\\faEnvelope \\\\ gyanprakash.sde} \\\\ $|$ \\\\\\n\\\\href{tel:+919575765381}{\\\\faMobile \\\\ +91 9575765381} \\\\\\\\\\\\\\\\\\n\\\\end{tabularx}\\n\\n% SUMMARY\\n\\\\section{Summary}\\nHighly analytical and problem-solving AI Engineering Undergraduate from IIT Madras, adept at building agentic RAG workflows and end-to-end ML systems using LangGraph, FastAPI, and Vector DBs. Possessing strong DSA fundamentals (219+ LeetCode problems), proficient in reducing inference latency and deploying scalable AI solutions. Eager to apply Generative AI and backend optimization skills in an SDE/AI internship, leveraging experience in simplifying complex AI concepts and mentoring peers, as demonstrated through project leadership and clear technical communication.\\n\\n% EDUCATION\\n\\\\section{Education}\\n\\\\begin{tabularx}{\\\\linewidth}{@{}X r@{}}\\n\\\\textbf{Indian Institute of Technology (IIT), Madras} & Chennai, Tamil Nadu \\\\\\\\\\\\\\\\\\n\\\\textit{BS in Data Science and Applications; CGPA: 8.18/10} & \\\\textit{2023 -- 2027} \\\\\\\\\\\\\\\\\\n\\\\end{tabularx}\\n\\n% SKILLS\\n\\\\section{Skills}\\n\\\\begin{tabularx}{\\\\linewidth}{@{}l X@{}}\\n\\\\textbf{Programming Languages} & \\\\normalsize{Python, JavaScript (ES6+), Java, SQL} \\\\\\\\\\\\\\\\\\n\\\\textbf{Backend Frameworks} & \\\\normalsize{Flask, FastAPI, Node.js} \\\\\\\\\\\\\\\\\\n\\\\textbf{Frontend Technologies} & \\\\normalsize{React, Vue.js, Bootstrap, Tailwind CSS} \\\\\\\\\\\\\\\\\\n\\\\textbf{Databases \\\\& Storage Systems} & \\\\normalsize{PostgreSQL, SQLite, DuckDB, Redis} \\\\\\\\\\\\\\\\\\n\\\\textbf{Vector Databases \\\\& Search} & \\\\normalsize{FAISS, ChromaDB} \\\\\\\\\\\\\\\\\\n\\\\textbf{Machine Learning \\\\& NLP} & \\\\normalsize{NumPy, Pandas, Scikit-learn, PyTorch, TensorFlow, NLTK} \\\\\\\\\\\\\\\\\\n\\\\textbf{Data Visualization} & \\\\normalsize{Power BI, Matplotlib, Seaborn, Plotly} \\\\\\\\\\\\\\\\\\n\\\\textbf{Generative AI \\\\& LLM Frameworks} & \\\\normalsize{LangChain, LangGraph, LangSmith, AGNO, Gemini (Flash-Lite, 1.5 Pro), OpenAI (GPT-3/4), LLaMA, Mistral, Fine-tuning (PEFT, LoRA)} \\\\\\\\\\\\\\\\\\n\\\\textbf{DevOps \\\\& Systems} & \\\\normalsize{Linux, Git, GitHub, Celery} \\\\\\\\\\\\\\\\\\n\\\\textbf{Cloud Platforms} & \\\\normalsize{AWS (EC2, S3, Lambda), GCP} \\\\\\\\\\\\\\\\\\n\\\\end{tabularx}\\n\\n% PROJECTS\\n\\\\section{Projects}\\n\\n\\\\begin{joblong}{YouTube RAG Chatbot \\\\textmd{$|$ LangGraph, FastAPI, FAISS, AsyncSQLite, Vue.js, Gemini (Flash-Lite)}}{\\\\href{https://github.com/gyanprakashkushwaha/youtube-rag-chatbot}{Repo Link}}\\n\\\\item Reduced Time-to-First-Token (TTFT) by 90\\\\% (from 5s to $<$500ms) by engineering a real-time async streaming pipeline with FastAPI and LangGraph, significantly improving user perceived responsiveness.\\n\\\\item Architected a stateful RAG workflow using LangGraph and AsyncSQLite to orchestrate persistent, multi-turn conversations, enabling context coherence across 1-hour+ video transcripts.\\n\\\\item Increased RAG response accuracy by implementing Maximal Marginal Relevance (MMR) search, filtering 90\\\\% of redundant context from long-form audio transcripts to minimize LLM hallucinations.\\n\\\\end{joblong}\\n\\n\\\\begin{joblong}{CropGuardian-AI \\\\textmd{$|$ LangGraph, FastAPI, Pydantic, Gemini 1.5 Pro, OpenMeteo}}{\\\\href{https://github.com/gyanprakashkushwaha/cropguardian-ai}{Repo Link}}\\n\\\\item Architected a multi-agent workflow using LangGraph to orchestrate 3 specialized agents (Vision, Weather, Agronomy), automating the complex reasoning chain from image analysis to actionable advice with $<$3s latency.\\n\\\\item Implemented strict Structured Output enforcement using Pydantic and Gemini 1.5 Pro, achieving 100\\\\% schema compliance for JSON responses and eliminating parsing errors in the frontend application.\\n\\\\item Designed a context-aware reasoning engine that grounds visual diagnosis in real-time environmental data (OpenMeteo API), generating location-specific farming action plans based on 7-day weather forecasts.\\n\\\\end{joblong}\\n\\n\\\\begin{joblong}{Customer Churn Predictor \\\\textmd{$|$ Python, Scikit-learn, XGBoost, MLflow, Streamlit}}{\\\\href{https://github.com/gyanprakashkushwaha/customer-churn-predictor}{Repo Link}}\\n\\\\item Developed a modular machine learning pipeline to process 100,000 customer records, benchmarking performance across 4 ensemble algorithms (XGBoost, Random Forest, AdaBoost, Gradient Boosting) to identify optimal churn predictors and optimize model performance for actionable insights.\\n\\\\item Engineered a robust training workflow integrated with MLflow to systematically track model metrics, parameters, and version history, ensuring reproducibility across experimentation cycles.\\n\\\\item Deployed the best-performing model as an interactive web application using Streamlit, facilitating understanding for non-technical stakeholders to input customer demographics and generate real-time churn risk assessments, effectively bridging technical insights with business needs.\\n\\\\end{joblong}\\n\\n\\\\begin{joblong}{Movie Recommender System \\\\textmd{$|$ Python, Scikit-Learn, Streamlit, Pandas}}{\\\\href{https://github.com/gyanprakashkushwaha/movie-recommender-system}{Repo Link}}\\n\\\\item Developed a Content-Based Movie Recommender System using Python and Streamlit, processing a dataset of 4,800+ movies to generate highly personalized top-10 viewing suggestions, enhancing user engagement.\\n\\\\item Engineered a feature extraction pipeline with Pandas and Scikit-Learn, transforming unstructured metadata (genres, cast, crew) into a 5,000-feature Bag-of-Words model to calculate Cosine Similarity scores.\\n\\\\item Deployed an interactive web application integrating the TMDB API to fetch real-time posters, utilizing Pickle serialization to optimize data loading and deliver recommendations efficiently.\\n\\\\end{joblong}\\n\\n% ACHIEVEMENTS\\n\\\\section{Achievements}\\n\\\\begin{itemize}[leftmargin=*]\\n  \\\\item \\\\textbf{Kaggle Expert}: Top 4\\\\% globally (Rank 341), 1 Silver \\\\& 9 Bronze medals; datasets with 22K+ views and 5.6K+ downloads\\n  \\\\item \\\\textbf{LeetCode}: Solved 219+ DSA problems (100+ Medium)\\n  \\\\item \\\\textbf{HackerRank}: 5$\\\\star$Gold Badge (SQL)\\n\\\\end{itemize}\\n\\n\\\\end{document}'}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Dict, Literal, Annotated\n",
    "from operator import add\n",
    "from pydantic import BaseModel, Field\n",
    "from app.prompts import ResumeBuilderPrompt\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "print(load_dotenv())\n",
    "# FIXME - Use other gemini model and then try to generate the latex code.\n",
    "# state\n",
    "\n",
    "class LatexCodeAnalysis(BaseModel):\n",
    "    decision: Literal[\"CORRECT\", \"NEEDS_IMPROVEMENT\"] = Field(...)\n",
    "    feedback: str = Field(..., description= \"something something\")\n",
    "\n",
    "class BuilderStateInput(TypedDict):\n",
    "    resume_text : str\n",
    "    analysis_result: str\n",
    "    \n",
    "class BuilderGenerationState(BuilderStateInput):\n",
    "    latex_resume_code: str\n",
    "    latex_code_evaluations : Annotated[list[str], add]\n",
    "    decision : Literal[\"CORRECT\", \"NEEDS_IMPROVEMENT\"]\n",
    "    \n",
    "class BuilderStateOutput(TypedDict):\n",
    "    latex_resume_code: str\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", api_key=os.getenv(\"GOOGLE_API_KEY_1\"))\n",
    "# Node Functions\n",
    "def builder_node(state: BuilderStateInput):\n",
    "    prompt = ResumeBuilderPrompt.BUILDER_PROMPT.format(\n",
    "        resume_text = state[\"resume_text\"],\n",
    "        analysis_report_json = json.loads(state[\"analysis_result\"])\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"latex_resume_code\": response.content}\n",
    "\n",
    "def evaluate_code(state: BuilderGenerationState):\n",
    "    prompt = ResumeBuilderPrompt.EVALUATOR_PROMPT.format(\n",
    "      latex_resume_code = state[\"latex_resume_code\"]\n",
    "    )\n",
    "    response = llm.with_structured_output(LatexCodeAnalysis).invoke(prompt)\n",
    "    return {\"latex_code_evaluations\": [response.feedback], \"decision\": response.decision}\n",
    "\n",
    "# Conditional func\n",
    "def evaluation_condition(state: BuilderGenerationState) -> Literal[\"CORRECT\", \"NEEDS_IMPROVEMENT\"]:\n",
    "    return state[\"decision\"]\n",
    "  \n",
    "\n",
    "# GRAPH\n",
    "builder = StateGraph(BuilderGenerationState, input_schema= BuilderStateInput, output_schema = BuilderStateOutput)\n",
    "# node\n",
    "builder.add_node(\"builder_node\", builder_node)\n",
    "builder.add_node(\"evaluate_code\", evaluate_code)\n",
    "\n",
    "# edges\n",
    "builder.add_edge(START, \"builder_node\")\n",
    "builder.add_edge(\"builder_node\", \"evaluate_code\")\n",
    "builder.add_conditional_edges(\"evaluate_code\", evaluation_condition, {\"CORRECT\": END, \"NEEDS_IMPROVEMENT\": \"builder_node\"})\n",
    "\n",
    "workflow = builder.compile()\n",
    "# workflow\n",
    "\n",
    "input_state = {\n",
    "    \"resume_text\": resume_text,\n",
    "    \"analysis_result\": json.dumps(analysis_output)\n",
    "}\n",
    "\n",
    "output = workflow.invoke(input_state)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2f2a34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ats_result': {'match_score': 85,\n",
       "  'missing_keywords': ['LLMs (Open AI, LLaMA, Mistral, etc.)',\n",
       "   'Fine-tuning / PEFT concepts'],\n",
       "  'formatting_issues': [],\n",
       "  'decision': 'PASS',\n",
       "  'feedback': 'The resume has a good match with the JD. However, it is missing specific mentions of LLM models like OpenAI, LLaMA, Mistral, and fine-tuning concepts like PEFT. Adding these to the skills or projects section would strengthen the application further. The resume is well-formatted and easy to parse.'},\n",
       " 'recruiter_result': {'career_progression_score': 95,\n",
       "  'red_flags': [],\n",
       "  'soft_skills_detected': ['analytical thinking', 'problem-solving skills'],\n",
       "  'decision': 'PASS',\n",
       "  'feedback': 'The candidate has a strong academic background and relevant project experience. The resume is well-structured and highlights key skills and achievements effectively. While the ATS noted a lack of specific LLM model mentions, the overall profile is very promising. Recommend moving forward.'},\n",
       " 'hm_result': {'tech_depth_score': 75,\n",
       "  'project_impact_score': 60,\n",
       "  'stack_alignment': 'Good',\n",
       "  'decision': 'MAYBE',\n",
       "  'interview_questions': ['Can you elaborate on your experience with fine-tuning LLMs? Specifically, what PEFT techniques have you used, and what were the challenges you faced in terms of data preparation and evaluation?',\n",
       "   'Describe a situation where you had to explain a complex AI concept (like RAG or prompt engineering) to someone with a non-technical background. What approach did you take, and what was the outcome?',\n",
       "   \"You mention building 'production-ready ML pipelines'. Can you walk me through the architecture of one such pipeline, highlighting the components for data ingestion, preprocessing, model training, deployment, and monitoring?\",\n",
       "   \"The JD emphasizes 'AI application architecture'. Can you discuss a project where you were responsible for the overall architecture of an AI application, detailing the trade-offs you considered in your design choices?\"],\n",
       "  'feedback': \"The candidate has a solid foundation in ML and data science, with good project experience in areas relevant to the JD, such as churn prediction and recommendation systems. Their YouTube RAG Chatbot project demonstrates experience with modern Generative AI concepts like RAG, LangGraph, and FastAPI, which aligns well with the JD's requirements. The candidate also shows good impact in terms of latency reduction (90% TTFT). However, the JD specifically asks for experience with LLMs like OpenAI, LLaMA, Mistral, and fine-tuning/PEFT concepts. While the candidate mentions RAG and Gemini (Flash-Lite), direct experience with fine-tuning specific LLMs or a broader range of LLM providers isn't explicitly detailed in the projects. The 'Stack Compliance' is rated 'Good' because the core technologies like Python, Scikit-learn, FastAPI, and concepts like RAG are present, but there's room for deeper alignment on specific LLM models and fine-tuning. To improve, the candidate should quantify the impact of their projects more, if possible (e.g., 'improved recommendation accuracy by X%', 'reduced false positives in churn prediction by Y%'). For the 'AI Instructor / Generative AI Mentor' role, emphasizing the teaching and mentoring aspects more explicitly in their summary or project descriptions would be beneficial. For instance, how they broke down complex concepts for others or mentored junior team members.\"}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_str = json.dumps(analysis_output)\n",
    "json.loads(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d29c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'latex_resume_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m input_state \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: resume_text,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis_result\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(analysis_output)\n\u001b[0;32m      6\u001b[0m }\n\u001b[1;32m----> 8\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\pregel\\main.py:3068\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[0;32m   3065\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3066\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 3068\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3076\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3078\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3080\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3082\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3083\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\pregel\\main.py:2643\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2641\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2642\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2643\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2644\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2648\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmpty\u001b[49m\n\u001b[0;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[1;32mc:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 656\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 400\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[42], line 42\u001b[0m, in \u001b[0;36mevaluate_code\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_code\u001b[39m(state: BuilderStateInput):\n\u001b[0;32m     41\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m ResumeBuilderPrompt\u001b[38;5;241m.\u001b[39mEVALUATOR_PROMPT\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m---> 42\u001b[0m       latex_resume_code \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatex_resume_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     43\u001b[0m     )\n\u001b[0;32m     44\u001b[0m     response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(LatexCodeAnalysis)\u001b[38;5;241m.\u001b[39minvoke(prompt)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatex_code_evaluations\u001b[39m\u001b[38;5;124m\"\u001b[39m: [response\u001b[38;5;241m.\u001b[39mfeedback], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39mdecision}\n",
      "\u001b[1;31mKeyError\u001b[0m: 'latex_resume_code'",
      "\u001b[0mDuring task with name 'evaluate_code' and id 'a11cd091-0094-2ccf-b32e-0861a6a3c514'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
