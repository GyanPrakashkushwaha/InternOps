{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0961934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "class ATSAnalysis(BaseModel):\n",
    "    match_score: int = Field(..., description=\"0-100 score based on keyword overlapping and hard constraints.\")\n",
    "    missing_keywords: List[str] = Field(..., description=\"Critical keywords from JD missing in Resume.\")\n",
    "    formatting_issues: List[str] = Field(..., description=\"Issues like complex tables, missing headers, or unparseable sections.\")\n",
    "    decision: Literal[\"PASS\", \"FAIL\"] = Field(..., description=\"If score < 70 or hard constraints missing, FAIL.\")\n",
    "    feedback: str = Field(..., description=\"Actionable advice to improve ATS readability.\")  \n",
    "\n",
    "class RecruiterAnalysis(BaseModel):\n",
    "    career_progression_score: int = Field(..., description=\"0-100 score on logical role transitions.\")\n",
    "    red_flags: List[str] = Field(..., description=\"Gaps > 6 months, job hopping, or downgrades in titles.\")\n",
    "    soft_skills_detected: List[str] = Field(..., description=\"Communication, leadership, or teamwork mentioned.\")\n",
    "    decision: Literal[\"PASS\", \"FAIL\"] = Field(..., description=\"Pass if no major red flags and clear progression.\")\n",
    "    feedback: str = Field(..., description=\"Advice on how the candidate presents their story.\")\n",
    "\n",
    "class HiringManagerAnalysis(BaseModel):\n",
    "    tech_depth_score: int = Field(..., description=\"0-100. Do they use 'built/architected' vs 'used'?\")\n",
    "    project_impact_score: int = Field(..., description=\"0-100. Are there metrics ($, %, users, latency)?\")\n",
    "    stack_alignment: str = Field(..., description=\"How well their specific tech experience maps to the JD's stack.\")\n",
    "    decision: Literal[\"HIRE\", \"NO_HIRE\", \"MAYBE\"]\n",
    "    interview_questions: List[str] = Field(..., description=\"3-5 hard technical questions to verify their claims.\")\n",
    "    feedback: str = Field(..., description=\"Deep technical advice on improving bullet points.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfce029f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'match_score': 20,\n",
       " 'missing_keywords': ['hey'],\n",
       " 'formatting_issues': ['something'],\n",
       " 'decision': 'PASS',\n",
       " 'feedback': 'nice nice.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recruit = {\n",
    "    \"ats\": ATSAnalysis(match_score=20,\n",
    "                       missing_keywords=['hey'],\n",
    "                       formatting_issues = [\"something\"],\n",
    "                       decision= \"PASS\",\n",
    "                       feedback = \"nice nice.\"\n",
    "                       ),\n",
    "    \n",
    "}\n",
    "if \"hm\" in recruit:\n",
    "    print(\"hyes\")\n",
    "recruit['ats'].model_dump()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
