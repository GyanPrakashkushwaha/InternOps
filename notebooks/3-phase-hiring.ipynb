{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c3b716",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ATSAnalysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_state = {\u001b[33m'\u001b[39m\u001b[33mresume_text\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mGyan Prakash Kushwaha\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m/githubGitHub| /linkedinLinkedIn| /gl⌢beKaggle| /envel⌢pegyanprakash.sde| ♂¶obile+91 9575765381\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSUMMARY\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAI Engineering Undergraduate (IIT Madras) with strong DSA fundamentals (200+ LeetCodeproblems). Experi-\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33menced in buildingagentic RAG workflowsand end-to-end ML systems usingLangGraph, FastAPI, and Vector\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDBs. Proficient in reducing inference latency and deploying scalable AI solutions. Seeking an SDE/AI internship to\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mleverage skills in Generative AI and backend optimization.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEDUCATION\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIndian Institute of Technology (IIT), MadrasChennai, Tamil Nadu\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBS in Data Science and Applications; CGPA: 8.18/10 2023 – 2027\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSKILLS\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProgramming Languages Python, JavaScript (ES6+), Java, SQL\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBackend Frameworks Flask, FastAPI, Node.js\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFrontend Technologies React, Vue.js, Bootstrap, Tailwind CSS\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDatabases & Storage Systems PostgreSQL, SQLite, DuckDB, Redis\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mVector Databases & Search FAISS, ChromaDB\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMachine Learning & NLP NumPy, Pandas, Scikit-learn, PyTorch, TensorFlow, NLTK\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mData Visualization Power BI, Matplotlib, Seaborn, Plotly\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerative AI & LLM Frameworks LangChain, LangGraph, LangSmith, AGNO\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDevOps & Systems Linux, Git, GitHub, Celery\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCloud Platforms AWS (EC2, S3, Lambda), GCP\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPROJECTS\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mYouTube RAG Chatbot|LangGraph, FastAPI, FAISS, AsyncSQLite, Vue.js, Gemini (Flash-Lite) Repo Link\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m– ReducedTime-to-First-Token (TTFT) by 90\u001b[39m\u001b[33m%\u001b[39m\u001b[33m(from 5s to<500ms) by engineering a real-time async streaming\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mpipeline withFastAPIandLangGraph, significantly improving user perceived responsiveness.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m– Architected astateful RAG workflowusingLangGraphandAsyncSQLiteto orchestrate persistent, multi-turn\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mconversations, enabling context coherence across1-hour+ video transcripts.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m– Increased RAG response accuracy by implementingMaximal Marginal Relevance (MMR)search, filtering\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m90\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[33mf redundant contextfrom long-form audio transcripts to minimizeLLM hallucinations.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCropGuardian-AI|LangGraph, FastAPI, Pydantic, Gemini 1.5 Pro, OpenMeteo Repo Link\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m– Architected amulti-agent workflowusingLangGraphto orchestrate3 specialized agents(Vision, Weather,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAgronomy), automating the complex reasoning chain from image analysis to actionable advice with<3s latency.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m– Implemented strictStructured Outputenforcement usingPydanticandGemini 1.5 Pro, achieving100\u001b[39m\u001b[33m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mschema compliancefor JSON responses and eliminating parsing errors in the frontend application.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m– Designed acontext-aware reasoning enginethat grounds visual diagnosis in real-time environmental data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m(OpenMeteo API), generating location-specific farming action plans based on7-day weather forecasts.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCustomer Churn Predictor|Python, Scikit-learn, XGBoost, MLflow, Streamlit Repo Link\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m– Developed a modular machine learning pipeline to process100,000 customer records, benchmarking perfor-\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mmance across4 ensemble algorithms(XGBoost, Random Forest, AdaBoost, Gradient Boosting) to identify op-\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mtimal churn predictors.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m– Engineered a robust training workflow integrated withMLflowto systematically track model metrics, parameters,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mand version history, ensuringreproducibilityacross experimentation cycles.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m– Deployed the best-performing model as an interactive web application usingStreamlit, enabling non-technical\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mstakeholders to input customer demographics and generatereal-time churn risk assessments.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMovie Recommender System|Python, Scikit-Learn, Streamlit, Pandas Repo Link\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m–Developed a Content-Based Movie Recommender SystemusingPythonandStreamlit, processing a dataset\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mof4,800+ moviesto generate personalized top-10 viewing suggestions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m–Engineered a feature extraction pipelinewithPandasandScikit-Learn, transforming unstructured metadata\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m(genres, cast, crew) into a5,000-featureBag-of-Words model to calculateCosine Similarityscores.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m–Deployed an interactive web applicationintegrating theTMDB APIto fetch real-time posters, utilizingPickle\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mserialization to optimize data loading and deliver recommendations efficiently.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mACHIEVEMENTS\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m•Kaggle Expert: Top 4\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[33mlobally (Rank 341), 1 Silver & 9 Bronze medals; datasets with 22K+ views and 5.6K+\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mdownloads\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m•LeetCode: Solved 219+ DSA problems (100+ Medium)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m•HackerRank: 5⋆Gold Badge (SQL)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mjob_description\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mJob Description\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mJob Title: Data Science & AI Intern  \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCompany: GEODISHA \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLocation: Hyderabad - Onsite \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDuration: 1- 4 Months \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAbout GEODISHA \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAt GEODISHA, we are at the forefront of Data Analytics and AI, leveraging data to solve  complex problems and drive innovation. Our team is a dedicated group of researchers, engineers,  and strategists who believe in the power of technology. We are passionately committed to  developing cutting-edge AI that is not only powerful but also ethical, transparent, and fair. We\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33mre  looking for the next generation of innovators to join us. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mThe Opportunity: This Isn\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33mt Your Typical Internship \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mWe are seeking truly exceptional interns to join our core Data & AI team. This is a unique  opportunity to move beyond theory and apply your skills to high-impact, real-world challenges  across the full data lifecycle. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mYou won\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33mt be on the sidelines. You\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33mll be paired with a senior mentor and embedded directly into  projects at the intersection of data engineering, data analytics, artificial intelligence, and human  behavior. We are looking for a candidate who is not just an outstanding programmer, but a  critical thinker who understands that great AI starts with great data, is passionate about the  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhy\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m behind the data, and sees the critical importance of building Responsible AI. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mWhat You’ll Do (Key Responsibilities): \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ Assist in developing AI-driven recommendation engines and personalization workflows. ∉ Analyze structured and unstructured datasets to derive insights and patterns. ∉ Work with senior team members on customer segmentation, predictive modeling, and LLM integrated analytics tools. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ Learn and support the development of modular data pipelines and models using industry  tools. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ Use Python, SQL, scikit-learn, and other libraries for experimentation and development. ∉ Present findings through visualization tools such as seaborn, matplotlib, or BI tools like  Power BI. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ Document code, experiments, and insights for further development. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mWho You Are (Our Ideal Candidate):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mWe are looking for a rising star who is driven, curious, and eager to make a tangible impact. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCore Requirements: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ Currently pursuing or recently completed a degree in Data Science, Computer Science,  Statistics, Mathematics, or related field. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ Good understanding of Python and SQL (R is a plus). \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ Familiarity with basic ML algorithms: classification, regression, clustering. ∉ Exposure to libraries such as pandas, NumPy, scikit-learn, etc. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ Interest in GenAI/NLP concepts like transformers or LLMs is a bonus. ∉ Strong analytical mindset, curiosity, and willingness to learn. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPassion & Interest (What Sets You Apart): \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ A genuine and demonstrable passion for the entire Data Analytics space, from robust  engineering to insightful analysis. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ Academic or project-based exposure to Behavioral Analytics or computational social  science. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ A strong, well-articulated interest in the field of Responsible AI, ethics, and algorithmic  fairness \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBonus Points: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ Exposure to tools like TensorFlow, Hugging Face, or BI dashboards. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m∉ Basic understanding of cloud platforms (AWS/GCP/Azure), Git, or APIs. ∉ Academic projects, personal experiments, or GitHub repositories demonstrating interest in  AI/ML. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mWhat You’ll Gain: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m● Hands-on experience in a startup environment working on cutting-edge AdTech & MarTech  products. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m● Mentorship from senior Data Science and AI professionals. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m● A chance to convert the internship into a full-time position based on performance.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mType of Opportunity: Internship \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mJob Title: Data Science & AI Intern\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\u2002\u001b[39;00m\u001b[38;5;130;01m\\u2002\u001b[39;00m\u001b[38;5;130;01m\\u2002\u001b[39;00m\u001b[38;5;130;01m\\u2002\u001b[39;00m\u001b[38;5;130;01m\\u2002\u001b[39;00m\u001b[38;5;130;01m\\u2002\u001b[39;00m\u001b[38;5;130;01m\\u2002\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSelection Process: Technical Interview + Personal Interview\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMonthly Stipend / Annual CTC: INR 25,000 - 30,000 / month\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInternship Duration: 1 - 4 months\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProposed Start Date: December 1, 2025\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mWork Hours: 11:00 AM - 8:00 PM\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMode of Work: In-person \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPrimary Work Location: Hyderabad\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mImmediate Joining Requirement: Yes \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mHead Office Location: Hyderabad\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNumber of Open Positions: 2 \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease Note :\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mThe last date to apply for this opportunity is November 18, 2025 by 11 AM IST.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNot adhering to placement and internship policy rules will lead to necessary disciplinary actions by the committee.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mats_result\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mATSAnalysis\u001b[49m(match_score=\u001b[32m92\u001b[39m, missing_keywords=[\u001b[33m'\u001b[39m\u001b[33mR\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHugging Face\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBehavioral Analytics\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcomputational social science\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mResponsible AI\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33methics\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33malgorithmic fairness\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHyderabad - Onsite (explicit confirmation)\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m1-4 Months (explicit confirmation)\u001b[39m\u001b[33m'\u001b[39m], formatting_issues=[\u001b[33m'\u001b[39m\u001b[33mSpecial characters in contact information (e.g., ♂¶obile, gl⌢be, envel⌢pe) could be problematic for some parsers.\u001b[39m\u001b[33m'\u001b[39m], decision=\u001b[33m'\u001b[39m\u001b[33mPASS\u001b[39m\u001b[33m'\u001b[39m, feedback=\u001b[33m'\u001b[39m\u001b[33mYour resume demonstrates a strong match with the technical requirements, including extensive experience in Python, SQL, ML algorithms, GenAI/LLM frameworks, and relevant tools. To further improve ATS readability and keyword matching, consider explicitly stating your interest in \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponsible AI,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33methics,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\u001b[33malgorithmic fairness\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in your summary or project descriptions if applicable. Also, if you are open to it, explicitly state your availability for an \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnsite\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m internship in \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHyderabad\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m1-4 Months\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m within your objective or summary to align perfectly with the job description\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33ms specific constraints.\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mrecruiter_result\u001b[39m\u001b[33m'\u001b[39m: RecruiterAnalysis(career_progression_score=\u001b[32m90\u001b[39m, red_flags=[], soft_skills_detected=[\u001b[33m'\u001b[39m\u001b[33mCommunication\u001b[39m\u001b[33m'\u001b[39m], decision=\u001b[33m'\u001b[39m\u001b[33mPASS\u001b[39m\u001b[33m'\u001b[39m, feedback=\u001b[33m\"\u001b[39m\u001b[33mGyan presents a very strong profile for an SDE/AI internship. The resume is concise, well-structured, and includes a clear summary. There are no employment gaps or job hopping, as this is an undergraduate profile seeking an internship. The projects are highly relevant, technically sophisticated, and demonstrate a clear aptitude for Generative AI and backend optimization. The candidate effectively highlights quantifiable achievements (e.g., 90\u001b[39m\u001b[33m%\u001b[39m\u001b[33m TTFT reduction, 100\u001b[39m\u001b[38;5;132;01m% s\u001b[39;00m\u001b[33mchema compliance). The deployment of a model enabling non-technical stakeholders implies good communication skills in translating complex technical work for a broader audience. To further enhance the resume, consider explicitly mentioning your interest in \u001b[39m\u001b[33m'\u001b[39m\u001b[33mResponsible AI,\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33methics,\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33malgorithmic fairness\u001b[39m\u001b[33m'\u001b[39m\u001b[33m if applicable to align with potential company values. Additionally, as per the ATS feedback, clearly state your availability for an \u001b[39m\u001b[33m'\u001b[39m\u001b[33mOnsite\u001b[39m\u001b[33m'\u001b[39m\u001b[33m internship in \u001b[39m\u001b[33m'\u001b[39m\u001b[33mHyderabad\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for \u001b[39m\u001b[33m'\u001b[39m\u001b[33m1-4 Months\u001b[39m\u001b[33m'\u001b[39m\u001b[33m within your summary or objective to ensure perfect alignment with specific job description constraints.\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mhm_result\u001b[39m\u001b[33m'\u001b[39m: HiringManagerAnalysis(tech_depth_score=\u001b[32m90\u001b[39m, project_impact_score=\u001b[32m85\u001b[39m, stack_alignment=\u001b[33m\"\u001b[39m\u001b[33mExcellent. The candidate\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms experience with LangGraph, FastAPI, Vector DBs (FAISS, ChromaDB), PyTorch, TensorFlow, and specifically their GenAI projects (YouTube RAG, CropGuardian-AI) aligns perfectly with the JD\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms focus on AI-driven recommendation engines, LLM integrated analytics, and cutting-edge AI. Strong foundation in Python, SQL, Scikit-learn, Pandas, NumPy, visualization tools (Power BI, Matplotlib, Seaborn), and cloud platforms (AWS, GCP) covers all core requirements. Their achievements on Kaggle and LeetCode further demonstrate practical competence and problem-solving skills, exceeding typical intern expectations.\u001b[39m\u001b[33m\"\u001b[39m, decision=\u001b[33m'\u001b[39m\u001b[33mHIRE\u001b[39m\u001b[33m'\u001b[39m, interview_questions=[\u001b[33m'\u001b[39m\u001b[33mRegarding your YouTube RAG Chatbot, you mentioned orchestrating persistent, multi-turn conversations using LangGraph and AsyncSQLite. Can you elaborate on the specific challenges you faced in managing session state for a potentially high-volume application, and how you ensured data consistency and concurrency safety with AsyncSQLite?\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIn your Customer Churn Predictor project, you benchmarked four ensemble algorithms. Beyond standard metrics like accuracy or F1-score, what specific considerations did you make for model interpretability and potential biases in the churn predictions, especially since it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms used for real-time risk assessments by non-technical stakeholders?\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mThe GEODISHA job description emphasizes Responsible AI, ethics, and algorithmic fairness. Considering your hands-on experience with LLMs in CropGuardian-AI and YouTube RAG, what steps did you take, or would you take, to ensure the AI\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms outputs are ethical, transparent, and fair, particularly when providing sensitive advice (like farming plans) or generating summaries?\u001b[39m\u001b[33m\"\u001b[39m], feedback=\u001b[33m\"\u001b[39m\u001b[33mYour resume showcases strong technical capabilities and impactful projects. To further enhance your bullet points and demonstrate even greater competence, consider the following:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m1.  **Customer Churn Predictor**: While you mentioned benchmarking 4 ensemble algorithms to identify optimal churn predictors, quantify the *impact* of this identification. For example, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbenchmarking performance across 4 ensemble algorithms and identifying XGBoost as the optimal predictor with an X\u001b[39m\u001b[38;5;132;01m% u\u001b[39;00m\u001b[33mplift in precision, potentially saving Z\u001b[39m\u001b[38;5;132;01m% i\u001b[39;00m\u001b[33mn customer acquisition costs by proactively targeting at-risk users.\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m2.  **Movie Recommender System**: For \u001b[39m\u001b[33m'\u001b[39m\u001b[33moptimized data loading and deliver recommendations efficiently,\u001b[39m\u001b[33m'\u001b[39m\u001b[33m provide a specific metric. \u001b[39m\u001b[33m'\u001b[39m\u001b[33mOptimized data loading by utilizing Pickle serialization, reducing recommendation generation time by X\u001b[39m\u001b[33m%\u001b[39m\u001b[33m (e.g., from Yms to Zms), thus enhancing user experience.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Similarly, for \u001b[39m\u001b[33m'\u001b[39m\u001b[33mprocessing a dataset of 4,800+ movies,\u001b[39m\u001b[33m'\u001b[39m\u001b[33m consider adding user engagement or satisfaction metrics if available (e.g., \u001b[39m\u001b[33m'\u001b[39m\u001b[33machieved an average user satisfaction score of X\u001b[39m\u001b[33m%\u001b[39m\u001b[33m'\u001b[39m\u001b[33m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m3.  **General**: Where possible, tie technical achievements directly to business outcomes or user value, even if hypothetical for academic projects. For instance, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mArchitected X, leading to Y (quantifiable business/user benefit).\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)}\n",
      "\u001b[31mNameError\u001b[39m: name 'ATSAnalysis' is not defined"
     ]
    }
   ],
   "source": [
    "final_state = {'resume_text': 'Gyan Prakash Kushwaha\\n/githubGitHub| /linkedinLinkedIn| /gl⌢beKaggle| /envel⌢pegyanprakash.sde| ♂¶obile+91 9575765381\\nSUMMARY\\nAI Engineering Undergraduate (IIT Madras) with strong DSA fundamentals (200+ LeetCodeproblems). Experi-\\nenced in buildingagentic RAG workflowsand end-to-end ML systems usingLangGraph, FastAPI, and Vector\\nDBs. Proficient in reducing inference latency and deploying scalable AI solutions. Seeking an SDE/AI internship to\\nleverage skills in Generative AI and backend optimization.\\nEDUCATION\\nIndian Institute of Technology (IIT), MadrasChennai, Tamil Nadu\\nBS in Data Science and Applications; CGPA: 8.18/10 2023 – 2027\\nSKILLS\\nProgramming Languages Python, JavaScript (ES6+), Java, SQL\\nBackend Frameworks Flask, FastAPI, Node.js\\nFrontend Technologies React, Vue.js, Bootstrap, Tailwind CSS\\nDatabases & Storage Systems PostgreSQL, SQLite, DuckDB, Redis\\nVector Databases & Search FAISS, ChromaDB\\nMachine Learning & NLP NumPy, Pandas, Scikit-learn, PyTorch, TensorFlow, NLTK\\nData Visualization Power BI, Matplotlib, Seaborn, Plotly\\nGenerative AI & LLM Frameworks LangChain, LangGraph, LangSmith, AGNO\\nDevOps & Systems Linux, Git, GitHub, Celery\\nCloud Platforms AWS (EC2, S3, Lambda), GCP\\nPROJECTS\\nYouTube RAG Chatbot|LangGraph, FastAPI, FAISS, AsyncSQLite, Vue.js, Gemini (Flash-Lite) Repo Link\\n– ReducedTime-to-First-Token (TTFT) by 90%(from 5s to<500ms) by engineering a real-time async streaming\\npipeline withFastAPIandLangGraph, significantly improving user perceived responsiveness.\\n– Architected astateful RAG workflowusingLangGraphandAsyncSQLiteto orchestrate persistent, multi-turn\\nconversations, enabling context coherence across1-hour+ video transcripts.\\n– Increased RAG response accuracy by implementingMaximal Marginal Relevance (MMR)search, filtering\\n90% of redundant contextfrom long-form audio transcripts to minimizeLLM hallucinations.\\nCropGuardian-AI|LangGraph, FastAPI, Pydantic, Gemini 1.5 Pro, OpenMeteo Repo Link\\n– Architected amulti-agent workflowusingLangGraphto orchestrate3 specialized agents(Vision, Weather,\\nAgronomy), automating the complex reasoning chain from image analysis to actionable advice with<3s latency.\\n– Implemented strictStructured Outputenforcement usingPydanticandGemini 1.5 Pro, achieving100%\\nschema compliancefor JSON responses and eliminating parsing errors in the frontend application.\\n– Designed acontext-aware reasoning enginethat grounds visual diagnosis in real-time environmental data\\n(OpenMeteo API), generating location-specific farming action plans based on7-day weather forecasts.\\nCustomer Churn Predictor|Python, Scikit-learn, XGBoost, MLflow, Streamlit Repo Link\\n– Developed a modular machine learning pipeline to process100,000 customer records, benchmarking perfor-\\nmance across4 ensemble algorithms(XGBoost, Random Forest, AdaBoost, Gradient Boosting) to identify op-\\ntimal churn predictors.\\n– Engineered a robust training workflow integrated withMLflowto systematically track model metrics, parameters,\\nand version history, ensuringreproducibilityacross experimentation cycles.\\n– Deployed the best-performing model as an interactive web application usingStreamlit, enabling non-technical\\nstakeholders to input customer demographics and generatereal-time churn risk assessments.\\nMovie Recommender System|Python, Scikit-Learn, Streamlit, Pandas Repo Link\\n–Developed a Content-Based Movie Recommender SystemusingPythonandStreamlit, processing a dataset\\nof4,800+ moviesto generate personalized top-10 viewing suggestions.\\n–Engineered a feature extraction pipelinewithPandasandScikit-Learn, transforming unstructured metadata\\n(genres, cast, crew) into a5,000-featureBag-of-Words model to calculateCosine Similarityscores.\\n–Deployed an interactive web applicationintegrating theTMDB APIto fetch real-time posters, utilizingPickle\\nserialization to optimize data loading and deliver recommendations efficiently.\\nACHIEVEMENTS\\n•Kaggle Expert: Top 4% globally (Rank 341), 1 Silver & 9 Bronze medals; datasets with 22K+ views and 5.6K+\\ndownloads\\n•LeetCode: Solved 219+ DSA problems (100+ Medium)\\n•HackerRank: 5⋆Gold Badge (SQL)\\n', 'job_description': 'Job Description\\nJob Title: Data Science & AI Intern  \\n\\nCompany: GEODISHA \\n\\nLocation: Hyderabad - Onsite \\n\\nDuration: 1- 4 Months \\n\\nAbout GEODISHA \\n\\nAt GEODISHA, we are at the forefront of Data Analytics and AI, leveraging data to solve  complex problems and drive innovation. Our team is a dedicated group of researchers, engineers,  and strategists who believe in the power of technology. We are passionately committed to  developing cutting-edge AI that is not only powerful but also ethical, transparent, and fair. We\\'re  looking for the next generation of innovators to join us. \\n\\nThe Opportunity: This Isn\\'t Your Typical Internship \\n\\nWe are seeking truly exceptional interns to join our core Data & AI team. This is a unique  opportunity to move beyond theory and apply your skills to high-impact, real-world challenges  across the full data lifecycle. \\n\\nYou won\\'t be on the sidelines. You\\'ll be paired with a senior mentor and embedded directly into  projects at the intersection of data engineering, data analytics, artificial intelligence, and human  behavior. We are looking for a candidate who is not just an outstanding programmer, but a  critical thinker who understands that great AI starts with great data, is passionate about the  \"why\" behind the data, and sees the critical importance of building Responsible AI. \\n\\nWhat You’ll Do (Key Responsibilities): \\n\\n∉ Assist in developing AI-driven recommendation engines and personalization workflows. ∉ Analyze structured and unstructured datasets to derive insights and patterns. ∉ Work with senior team members on customer segmentation, predictive modeling, and LLM integrated analytics tools. \\n\\n∉ Learn and support the development of modular data pipelines and models using industry  tools. \\n\\n∉ Use Python, SQL, scikit-learn, and other libraries for experimentation and development. ∉ Present findings through visualization tools such as seaborn, matplotlib, or BI tools like  Power BI. \\n\\n∉ Document code, experiments, and insights for further development. \\n\\nWho You Are (Our Ideal Candidate):\\n\\nWe are looking for a rising star who is driven, curious, and eager to make a tangible impact. \\n\\nCore Requirements: \\n\\n∉ Currently pursuing or recently completed a degree in Data Science, Computer Science,  Statistics, Mathematics, or related field. \\n\\n∉ Good understanding of Python and SQL (R is a plus). \\n\\n∉ Familiarity with basic ML algorithms: classification, regression, clustering. ∉ Exposure to libraries such as pandas, NumPy, scikit-learn, etc. \\n\\n∉ Interest in GenAI/NLP concepts like transformers or LLMs is a bonus. ∉ Strong analytical mindset, curiosity, and willingness to learn. \\n\\nPassion & Interest (What Sets You Apart): \\n\\n∉ A genuine and demonstrable passion for the entire Data Analytics space, from robust  engineering to insightful analysis. \\n\\n∉ Academic or project-based exposure to Behavioral Analytics or computational social  science. \\n\\n∉ A strong, well-articulated interest in the field of Responsible AI, ethics, and algorithmic  fairness \\n\\nBonus Points: \\n\\n∉ Exposure to tools like TensorFlow, Hugging Face, or BI dashboards. \\n\\n∉ Basic understanding of cloud platforms (AWS/GCP/Azure), Git, or APIs. ∉ Academic projects, personal experiments, or GitHub repositories demonstrating interest in  AI/ML. \\n\\nWhat You’ll Gain: \\n\\n● Hands-on experience in a startup environment working on cutting-edge AdTech & MarTech  products. \\n\\n● Mentorship from senior Data Science and AI professionals. \\n\\n● A chance to convert the internship into a full-time position based on performance.\\n\\nType of Opportunity: Internship \\n\\nJob Title: Data Science & AI Intern\\n\\n\\u2002\\u2002\\u2002\\u2002\\u2002\\u2002\\u2002\\n\\nSelection Process: Technical Interview + Personal Interview\\n\\nMonthly Stipend / Annual CTC: INR 25,000 - 30,000 / month\\n\\nInternship Duration: 1 - 4 months\\n\\nProposed Start Date: December 1, 2025\\n\\nWork Hours: 11:00 AM - 8:00 PM\\n\\nMode of Work: In-person \\n\\nPrimary Work Location: Hyderabad\\n\\nImmediate Joining Requirement: Yes \\n\\nHead Office Location: Hyderabad\\n\\nNumber of Open Positions: 2 \\n\\nPlease Note :\\n\\nThe last date to apply for this opportunity is November 18, 2025 by 11 AM IST.\\nNot adhering to placement and internship policy rules will lead to necessary disciplinary actions by the committee.\\n', 'ats_result': ATSAnalysis(match_score=92, missing_keywords=['R', 'Hugging Face', 'Behavioral Analytics', 'computational social science', 'Responsible AI', 'ethics', 'algorithmic fairness', 'Hyderabad - Onsite (explicit confirmation)', '1-4 Months (explicit confirmation)'], formatting_issues=['Special characters in contact information (e.g., ♂¶obile, gl⌢be, envel⌢pe) could be problematic for some parsers.'], decision='PASS', feedback='Your resume demonstrates a strong match with the technical requirements, including extensive experience in Python, SQL, ML algorithms, GenAI/LLM frameworks, and relevant tools. To further improve ATS readability and keyword matching, consider explicitly stating your interest in \"Responsible AI,\" \"ethics,\" and \"algorithmic fairness\" in your summary or project descriptions if applicable. Also, if you are open to it, explicitly state your availability for an \"Onsite\" internship in \"Hyderabad\" for \"1-4 Months\" within your objective or summary to align perfectly with the job description\\'s specific constraints.'), 'recruiter_result': RecruiterAnalysis(career_progression_score=90, red_flags=[], soft_skills_detected=['Communication'], decision='PASS', feedback=\"Gyan presents a very strong profile for an SDE/AI internship. The resume is concise, well-structured, and includes a clear summary. There are no employment gaps or job hopping, as this is an undergraduate profile seeking an internship. The projects are highly relevant, technically sophisticated, and demonstrate a clear aptitude for Generative AI and backend optimization. The candidate effectively highlights quantifiable achievements (e.g., 90% TTFT reduction, 100% schema compliance). The deployment of a model enabling non-technical stakeholders implies good communication skills in translating complex technical work for a broader audience. To further enhance the resume, consider explicitly mentioning your interest in 'Responsible AI,' 'ethics,' and 'algorithmic fairness' if applicable to align with potential company values. Additionally, as per the ATS feedback, clearly state your availability for an 'Onsite' internship in 'Hyderabad' for '1-4 Months' within your summary or objective to ensure perfect alignment with specific job description constraints.\"), 'hm_result': HiringManagerAnalysis(tech_depth_score=90, project_impact_score=85, stack_alignment=\"Excellent. The candidate's experience with LangGraph, FastAPI, Vector DBs (FAISS, ChromaDB), PyTorch, TensorFlow, and specifically their GenAI projects (YouTube RAG, CropGuardian-AI) aligns perfectly with the JD's focus on AI-driven recommendation engines, LLM integrated analytics, and cutting-edge AI. Strong foundation in Python, SQL, Scikit-learn, Pandas, NumPy, visualization tools (Power BI, Matplotlib, Seaborn), and cloud platforms (AWS, GCP) covers all core requirements. Their achievements on Kaggle and LeetCode further demonstrate practical competence and problem-solving skills, exceeding typical intern expectations.\", decision='HIRE', interview_questions=['Regarding your YouTube RAG Chatbot, you mentioned orchestrating persistent, multi-turn conversations using LangGraph and AsyncSQLite. Can you elaborate on the specific challenges you faced in managing session state for a potentially high-volume application, and how you ensured data consistency and concurrency safety with AsyncSQLite?', \"In your Customer Churn Predictor project, you benchmarked four ensemble algorithms. Beyond standard metrics like accuracy or F1-score, what specific considerations did you make for model interpretability and potential biases in the churn predictions, especially since it's used for real-time risk assessments by non-technical stakeholders?\", \"The GEODISHA job description emphasizes Responsible AI, ethics, and algorithmic fairness. Considering your hands-on experience with LLMs in CropGuardian-AI and YouTube RAG, what steps did you take, or would you take, to ensure the AI's outputs are ethical, transparent, and fair, particularly when providing sensitive advice (like farming plans) or generating summaries?\"], feedback=\"Your resume showcases strong technical capabilities and impactful projects. To further enhance your bullet points and demonstrate even greater competence, consider the following:\\n1.  **Customer Churn Predictor**: While you mentioned benchmarking 4 ensemble algorithms to identify optimal churn predictors, quantify the *impact* of this identification. For example, 'benchmarking performance across 4 ensemble algorithms and identifying XGBoost as the optimal predictor with an X% uplift in precision, potentially saving Z% in customer acquisition costs by proactively targeting at-risk users.'\\n2.  **Movie Recommender System**: For 'optimized data loading and deliver recommendations efficiently,' provide a specific metric. 'Optimized data loading by utilizing Pickle serialization, reducing recommendation generation time by X% (e.g., from Yms to Zms), thus enhancing user experience.' Similarly, for 'processing a dataset of 4,800+ movies,' consider adding user engagement or satisfaction metrics if available (e.g., 'achieved an average user satisfaction score of X%').\\n3.  **General**: Where possible, tie technical achievements directly to business outcomes or user value, even if hypothetical for academic projects. For instance, 'Architected X, leading to Y (quantifiable business/user benefit).'\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3136438",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfinal_state\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'final_state' is not defined"
     ]
    }
   ],
   "source": [
    "final_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
